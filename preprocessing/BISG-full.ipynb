{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97294499-e814-4948-9924-22b26a0d48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from functools import reduce\n",
    "from typing import Iterable\n",
    "import math\n",
    "import tqdm\n",
    "from evaltools.data import census\n",
    "import us\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb95e6-44c6-4314-b9fb-9882e7c6408c",
   "metadata": {},
   "source": [
    "# In this first section of the notebook, I'm using census from evaltools to get VAP broken down by race for every block in Texas.\n",
    "\n",
    "1) I make the call to census using the P4 table which gets NH racial categories + HVAP\n",
    "2) I'm creating a mapping for each racial category in Texas to the list of columns in the returned dataframe. I ultimately only include (NH) BLACK, (NH) WHITE, HISP, (NH) ASIAN, (NH) OTHER\n",
    "3) I sum all of these columns to create find the probability that someone in a given block is a certain race. \n",
    "    * Here I used 2020 data\n",
    "    * There are also other combinations that can be made by calling census on multiple tables and combining. For example, calling P3 and P4 would give BLACK and (NH) BLACK, and by taking the difference we could find (H) Black. I have chosen not to do this, because the surname data doesn't have these breakdowns. \n",
    "4) I'm filling all nan-values with 0. \n",
    "5) Ultimately, I ran this and saved to a DataFrame, so this section doesn't have to be run each time \n",
    "\n",
    "** Mostly including to cover any methodology questions that may come up later. Note item number 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe042fbd-db76-40a7-ad1a-6137a0eb3c91",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m texas \u001b[38;5;241m=\u001b[39m us\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mTX\n\u001b[0;32m----> 2\u001b[0m texas_vap_p4 \u001b[38;5;241m=\u001b[39m \u001b[43mcensus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mP4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/evaltools/data/census.py:102\u001b[0m, in \u001b[0;36mcensus\u001b[0;34m(state, table, columns, geometry, key)\u001b[0m\n\u001b[1;32m     97\u001b[0m escaped \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param, value \u001b[38;5;129;01min\u001b[39;00m unescaped\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Send the request and create a dataframe.\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m req \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mescaped\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    103\u001b[0m header, data \u001b[38;5;241m=\u001b[39m req[\u001b[38;5;241m0\u001b[39m], req[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    104\u001b[0m chunk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mheader)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/requests/api.py:75\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    524\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 529\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/requests/sessions.py:687\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 687\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/requests/models.py:838\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/requests/models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 760\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    761\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/urllib3/response.py:572\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/urllib3/response.py:767\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m    769\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    770\u001b[0m )\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/urllib3/response.py:711\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left:\n\u001b[0;32m--> 711\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m-\u001b[39m amt\n\u001b[1;32m    713\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/http/client.py:626\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m s \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 626\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXAMOUNT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s), amt)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "texas = us.states.TX\n",
    "texas_vap_p4 = census(texas, table = \"P4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54b5bc-a621-4f4d-9bf3-6b02fbb679cd",
   "metadata": {},
   "source": [
    "This folder where this notebook is has the Texas_VAP_P4 csv, so the cell below can be changed to read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e371ff3-eba1-4115-a347-dd674a3e1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_vap_p4.to_csv(\"Texas_VAP_P4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4620ae-a171-4c74-b206-cacc91875244",
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_vap_p4 = pd.read_csv(\"Texas_VAP_P4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b5de8-7d14-4ce1-a4c8-db85e1a45682",
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_vap_p4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3133bc-a92f-4261-a972-115136e83845",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vap_cols = [col for col in texas_vap_p4.columns]\n",
    "black_cols = [col for col in all_vap_cols if \"BLACK\" in col]\n",
    "all_vap_cols = list(set(all_vap_cols) - set(black_cols))\n",
    "asian_cols = [col for col in all_vap_cols if \"ASIAN\" in col]\n",
    "all_vap_cols = list(set(all_vap_cols) - set(asian_cols))\n",
    "hisp_cols = [col for col in all_vap_cols if col == \"HVAP20\"]\n",
    "all_vap_cols = list(set(all_vap_cols) - set(hisp_cols))\n",
    "white_cols = [col for col in all_vap_cols if col == \"NHWHITEVAP20\"]\n",
    "all_vap_cols = list(set(all_vap_cols) - set(white_cols))\n",
    "oth_cols = [col for col in all_vap_cols if col != \"VAP20\" and col != \"GEOID20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900e3e7-5be7-412b-92a8-2f8a052207c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_mapping = {\"BLACK\": black_cols, \n",
    "                \"ASIAN\": asian_cols, \n",
    "                \"HISP\": hisp_cols, \n",
    "                \"WHITE\": white_cols, \n",
    "                \"OTH\": oth_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070ea6d-d1ce-4186-b441-dfd13a3bf947",
   "metadata": {},
   "outputs": [],
   "source": [
    "for race in race_mapping.keys():\n",
    "    texas_vap_p4[race] =  texas_vap_p4[race_mapping[race]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96c7df-56a1-4bed-a211-492991de95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = 0\n",
    "for race in race_mapping[\"OTH\"]:\n",
    "    tot_sum += sum(texas_vap_p4[race])\n",
    "assert tot_sum == sum(texas_vap_p4[\"OTH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c86ef9-e486-45de-9e7d-e5b63b1c7721",
   "metadata": {},
   "source": [
    "Here's a quick note to say that the BISG paper from JN and Bhushan necessarily use Pr(b|r), or given a block what is the likelihood of someone of that race living in that specific block. \n",
    "I believe the right calculation to do is the one below, but a second set of eyes on that wouldn't hurt!\n",
    "\n",
    "I did this calculation in a Texas-specific way, but in the paper it's done on a national level (?) I think, so also double checking that would be helpful!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498bd603-a585-4f1d-9930-88d2f98b519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for race in race_mapping.keys():\n",
    "    texas_vap_p4[f\"{race}_prob\"] = texas_vap_p4[race]/sum(texas_vap_p4[race])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1bf0a-2a95-4416-9c3d-fa2ed2a65abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for race in race_mapping.keys():\n",
    "    texas_vap_p4[f\"{race}_share\"] = texas_vap_p4[race]/texas_vap_p4[\"VAP20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bdd70a-1580-4d5f-8bde-48e667da3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert math.isclose(sum(texas_vap_p4[\"HISP_prob\"]), 1, abs_tol = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38bc62-76e4-4132-87b0-bec6efaf63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_prob_df = texas_vap_p4[[\"GEOID20\", \"BLACK_prob\", \"HISP_prob\", \"ASIAN_prob\", \"WHITE_prob\", \"OTH_prob\", \"BLACK_share\", \"HISP_share\", \"ASIAN_share\", \"WHITE_share\", \"OTH_share\"]]\n",
    "race_prob_df = race_prob_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc253486-3d29-4680-b70e-2ce0afbacfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_prob_df.to_csv(\"TX-block-race-prob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab69d8-04d0-4997-9f11-0bbb0e21ab59",
   "metadata": {},
   "source": [
    "Here's a quick check. I'll do further comparisons against the data.mggg texas block shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9a0d6-03de-4946-93ff-55cf4f741c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vap_blocks = len(texas_vap_p4[texas_vap_p4.VAP20 > 0])\n",
    "print(all_vap_blocks)\n",
    "for col in race_prob_df.columns: \n",
    "    if \"prob\" in col: \n",
    "        col_name = col.split(\"_\")[0]\n",
    "        print(f\"Share of populated blocks with {col_name} people: {len(race_prob_df[race_prob_df[col] > 0])/all_vap_blocks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183cb26-5ac4-4653-aff9-cd7b8a72b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_county = gpd.read_file(\"http://data.mggg.org.s3-website.us-east-2.amazonaws.com/census-2020/tx/tx_county.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2701f-7f4f-4776-8219-c74a976441b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_county[\"EIOTH20\"] = tx_county.VAP20 - tx_county.APBVAP20 - tx_county.HVAP20 - tx_county.ASIANVAP20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858fa4a-f608-4d7e-a7fe-0d2abf2d318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_county_dict = {\"BLACK\": \"APBVAP20\", \n",
    "                  \"HISP\": \"HVAP20\", \n",
    "                  \"ASIAN\": \"ASIANVAP20\",\n",
    "                  \"OTH\": \"EIOTH20\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cec71-72cb-498d-a121-649015b2975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tx_county_dict.keys():\n",
    "    print(f\"Group: {k}, Evaltools: {sum(texas_vap_p4[race_mapping[k]].sum(axis=1))/sum(texas_vap_p4.VAP20)}, data.mggg: {sum(tx_county[tx_county_dict[k]])/sum(tx_county.VAP20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474dd68c-4618-49eb-9715-5a030872dab4",
   "metadata": {},
   "source": [
    "Another 1 time grab!\n",
    "\n",
    "In this section I'm getting all of the surnames included in the census API (there are 162,253) which covers 90% of the people whose surnames were included at the time.\n",
    "\n",
    "Open to any testing suggestions, as well. This is just not a place where we really have ground truths to check against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489125f-a75d-428c-ae4b-8d7322d54de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.census.gov/data/2010/surname?get=NAME,'\n",
    "groups = {\"PCTWHITE\":\"WHITE\", \n",
    "          \"PCTBLACK\": \"BLACK\", \n",
    "          \"PCTHISPANIC\": \"HISP\", \n",
    "          \"PCTAPI\": \"ASIAN\", \n",
    "          \"PCTAIAN\": \"AMIN\", \n",
    "          \"PCT2PRACE\": \"2MORE\"}\n",
    "\n",
    "   \n",
    "for group in sorted(list(groups.keys()))[:-1]:\n",
    "        base_url += f\"{group},\"\n",
    "base_url += f\"{sorted(list(groups.keys()))[-1]}\"\n",
    "base_url += \"&RANK=1:1000000\"\n",
    "\n",
    "req = requests.get(base_url).json()\n",
    "\n",
    "pred_df = pd.DataFrame(req[1:], columns=req[0]).rename(columns=groups).drop(columns=[\"RANK\"])\n",
    "pred_df = pred_df.replace({'(S)':0})\n",
    "pred_df[\"OTH\"] = pred_df[\"AMIN\"].astype(float) + pred_df[\"2MORE\"].astype(float)\n",
    "pred_df = pred_df.drop(columns=[\"AMIN\", \"2MORE\"])\n",
    "pred_df.to_csv(\"All-Surname-probs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53486216-9595-44fb-bd42-10cfffeef2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv(\"All-Surname-probs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad15a4-f1c1-4e84-8c21-289e92e48ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pred_df.columns[1:]:\n",
    "    pred_df[col] = pred_df[col]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7db19f-46a3-4d3c-b43e-a403ad183855",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"All-Surname-probs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ef8a8-bd81-4d71-bdea-aa1f65e619d0",
   "metadata": {},
   "source": [
    "Prior to this, I'd already truncated the headers file to only include rows for Anderson county. \n",
    "This is for testing purposes only. I saved this to a tab separated value file, called `Anderson-Headers.tab`\n",
    "\n",
    "This version of anderson is from a cleaned file created by Max called `voterfile-2017` meaning this only includes elections that happened 2017 and before. (This isn't particularly relevant until later).\n",
    "\n",
    "In this section, I'm creating `anderson_mini`, a dataframe only including columns that will be relevant for testing. \n",
    "\n",
    "The header dataframe does have blocks, but the block ids included in that dataframe are not GEOIDs, so I'm also doing some of my own pre-processing to get map their block-ids to GEOID20s. This is how to read that\n",
    "* I'm pulling the Texas county shape from data.mggg\n",
    "* I'm creating a dictionary (saved in the first run), of county names to county fips\n",
    "* Reconstructing the GEOID20 column from the other included info\n",
    "    * Recall Block GEOID20 = (State FIPS) + (County FIPS) + (Tract ID) + (Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cda408-7b34-4f3e-a820-589d067f6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson = pd.read_csv(\"ANDERSON_Header.csv\", dtype={\"Residence_Addresses_CensusTract\":str, \"Residence_Addresses_CensusBlock\":str}).drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1516b9c-aa65-4604-b038-22dcfeb77623",
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9dcc8a-df8f-4eae-b6b8-977765f0893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total voters in Anderson:\", len(anderson))\n",
    "print(\"Total voters in unidentified tracts in Anderson:\", len(anderson[anderson[\"Residence_Addresses_CensusTract\"].isna()]))\n",
    "print(\"Total voters in unidentified blocks in Anderson:\", len(anderson[anderson[\"Residence_Addresses_CensusBlock\"].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48028a85-12ed-468a-bb23-c12664299deb",
   "metadata": {},
   "source": [
    "For my testing purposes, I'm dropping nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c149fa-062e-4380-adb4-2ccde0e3a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson = anderson.dropna(subset=['Residence_Addresses_CensusBlock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07672ac3-c135-487c-be3e-03f180c0a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_county = json.load(open(\"TX-county-name-fips.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272abe28-ef99-4689-aabb-ef959321dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson[\"COUNTYFP20\"] = len(anderson) * [tx_county[\"Anderson\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafd4c8-9101-4b91-b642-228dbf87709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson[\"STATEFP20\"] = len(anderson) * ['48']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb110017-ff30-4b0f-aa3f-f9454ea44955",
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson[\"GEOID20\"] = anderson[\"STATEFP20\"].astype(str) + anderson[\"COUNTYFP20\"].astype(str).str.zfill(3) + anderson[\"Residence_Addresses_CensusTract\"] + anderson[\"Residence_Addresses_CensusBlock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844cae6-f34b-410e-9c82-e57d83b6fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson_mini = anderson[['LALVOTERID', 'Voters_LastName', 'GEOID20']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259f652-d44d-48f3-9738-0c2cfba04ec6",
   "metadata": {},
   "source": [
    "Including these TX specific checks, will need help debugging what I'm seeing. \n",
    "\n",
    "Essentially, I'm reconstructing GEOIDs as above, it does seem to create 1 weird NaN geoid (even tho I fillna), but there are a number of GEOIDs in the header file that aren't in the tx_block shp\n",
    "\n",
    "The GEOIDs coming from the header file *look* valid, they just don't exist :/\n",
    "\n",
    "Also putting this here, but reading in the tx_block shp is sooo slow. I'd maybe do it AFTER the BISG section if you're reviewing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907e469-4abb-44b1-9182-a491d0721e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_block = gpd.read_file(\"http://data.mggg.org.s3-website.us-east-2.amazonaws.com/census-2020/tx/tx_block.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45022579-1c52-4eb4-afe4-dc0e2672df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tx_block[tx_block.COUNTYFP20 == \"001\"].GEOID20.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b55ec-ca1e-4ec5-8521-69eeac870e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(anderson_mini.GEOID20.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53f893-4c6c-4475-92ca-71b4834b4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Anderson: {len(anderson_mini.GEOID20.unique())}, TX Block: {len(tx_block[tx_block.COUNTYFP20 == '001'].GEOID20.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f15cff-e5bc-4ba4-9d24-b86f872fc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(anderson_mini.GEOID20.unique()) - set(tx_block[tx_block.COUNTYFP20 == \"001\"].GEOID20.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0ce86-4818-4438-8b45-3cbf680b4905",
   "metadata": {},
   "source": [
    "Ok, cool!\n",
    "\n",
    "(I'm typing in real time sorry)\n",
    " \n",
    "Now I have the race_prob_df and the surname_prob_df (and the anderson df). This is everything I need to do BISG!\n",
    "\n",
    "Quickly running into issues that voters are mapped to a block with 0 pop. Either need to re-get GEOIDs from lat-long for the voters or figure out cases like this. \n",
    "\n",
    "In the case where either the name is not included in the surname prediction csv (census only included names with at least 5 occurrences) OR the denominator is 0, meaning that there were no people in the block, I give an equal weight to all possibilities. \n",
    "\n",
    "Now, up until this point, I've been keeping white and other separate for qa purposes, however, when EI is run, these will be combined. Looking for feedback about whether I should combine them before this step OR at this step give BLACK = 0.25, HISP = 0.25, ASIAN = 0.25, WHITE = 0.125, and OTH = 0.125 for the purposes of recombining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c07a97-82c2-4992-903d-a04486d21d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv(\"All-Surname-probs.csv\")\n",
    "race_prob_df = pd.read_csv(\"TX-block-race-prob.csv\", dtype={\"GEOID20\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb5baf-8c01-4856-a5a8-c1e226dcdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_prob_df_test = race_prob_df[race_prob_df.BLACK_prob > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312cb71-15f5-45db-bbd7-ca01b787dd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845b116-8d1d-4f53-9fe7-0967e722d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_prob_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a64e9951-85a5-4443-bfc4-701f4820d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_pred_df = pd.DataFrame(columns=[\"LALVOTERID\", \"LASTNAME\", \"GEOID20\", \"BLACK_prob\", \"HISP_prob\", \"ASIAN_prob\", \"WHITE_prob\", \"OTH_prob\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75799409-dd48-4bc4-8f5b-e624f5532b48",
   "metadata": {},
   "source": [
    "# OK making bigger text here for something worth noting\n",
    "There are 1211 unique GEOIDs found for Anderson. \n",
    "Of those only 841 are represented in the race_prob_df GEOIDs. \n",
    "This ends up with ~13000 out of ~25000 voters being represented. So HALF of the voters could be lost due to random GEOIDs. \n",
    "\n",
    "For now, I'll just fill those with 25% chance of being any of the racial groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73e49cdf-3fc8-4675-93f2-c297f09584de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anderson.GEOID20.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df4368c9-30ad-474c-bfa0-9639da46d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for geoid in anderson.GEOID20.unique():\n",
    "    assert len(geoid) == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99585f68-ac04-443c-9a3d-a9d9225cd1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anderson.GEOID20[anderson.GEOID20.isin(race_prob_df.GEOID20)].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "757b4586-0c2b-4053-b0d1-d3749042bb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1211-841"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4c953-91ca-43b1-b197-936d9dd61700",
   "metadata": {},
   "source": [
    "# And now...BISG\n",
    "\n",
    "I'm sure there's a smarter way to do this so it isn't as slow considering we'll have to do all counties. \n",
    "\n",
    "You can see the assertion statement included to ensure that the sum of final probabilities for each voter is close to 1. \n",
    "The reason for the is close instead of 1 is because I've seen some .9999.... or 1.0000.... values which I think still means the probability\n",
    "is valid, and the point of error is just floating point stuff.\n",
    "\n",
    "This took 40 minutes to run on Anderson county (25,000 voters-ish), so take a look at this, but I saved the outputs and read them in below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ced16d2d-ed05-4b18-8ff5-c3e93ad4fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "andrews = pd.read_csv(\"Andrews-header16.csv\", dtype={\"GEOID20\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bae00cf2-b2d9-40ac-ae5b-da090f4f3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_prob_df.GEOID20 = race_prob_df.GEOID20.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd06249a-b1c7-4a00-bce1-99a41ce9a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:01, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:01, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "NAME: Avitia Fernandez\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:02, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:02, 16.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Garcia Tamayo\n",
      "NAME: Wyatt-Warren\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:02, 21.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "NAME: Ho-Gland\n",
      "NAME: Klebahn\n",
      "NAME: Silva De Luna\n",
      "NAME: Luna Ortiz\n",
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:03, 17.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:03, 14.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:04, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [00:05, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:06, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:06, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [00:07, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [00:09, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:12,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Madrilez\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [00:13, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [00:14, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n",
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [00:14, 14.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [00:15, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:15, 14.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Hoyl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "216it [00:17, 12.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:17, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Emsoff\n",
      "NAME: Emsoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "233it [00:18, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239it [00:18, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Perez-Cordova\n",
      "NAME: Garza-Haro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "253it [00:20, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [00:22, 16.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Jefcoats\n",
      "NAME: Jefcoats\n",
      "NAME: Jefcoats\n",
      "NAME: Jefcoats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [00:23, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [00:23, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [00:25, 14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n",
      "GEOID20\n",
      "NAME: Elcewicz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "334it [00:26, 12.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [00:26, 12.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ix, row in tqdm.tqdm(andrews.iterrows()):\n",
    "    voter_id = row[\"LALVOTERID\"]\n",
    "    name = row[\"Voters_LastName\"]\n",
    "    block_prob = race_prob_df[race_prob_df.GEOID20 == row.GEOID20]\n",
    "    \n",
    "    if name.upper() in list(pred_df.NAME):\n",
    "        surname_prob = pred_df[[\"BLACK\", \"HISP\", \"ASIAN\", \"WHITE\", \"OTH\"]][pred_df.NAME == name.upper()]\n",
    "        voter_dict = {}\n",
    "        for race in surname_prob.columns:\n",
    "            voter_dict[race] = (float(surname_prob[race].values[0]) * float(block_prob[f\"{race}_prob\"].values[0]))\n",
    "        denom = sum(voter_dict.values())\n",
    "        if denom > 0: \n",
    "            voter_dict = {k: v/denom for k, v in voter_dict.items()}\n",
    "            new_row = [voter_id, name, row.GEOID20, voter_dict[\"BLACK\"], voter_dict[\"HISP\"], voter_dict[\"ASIAN\"], voter_dict[\"WHITE\"], voter_dict[\"OTH\"]]\n",
    "        else: \n",
    "            print(\"denominator not greater than 0\")\n",
    "            new_row = [voter_id, name, row.GEOID20, block_prob.BLACK_share, block_prob.HISP_share, block_prob.ASIAN_share, block_prob.WHITE_share, block_prob.OTH_share] \n",
    "    else: \n",
    "        if name.upper() not in list(pred_df.NAME): \n",
    "                print(f\"NAME: {name}\")\n",
    "        else: \n",
    "            print(\"GEOID20\")\n",
    "        new_row = [voter_id, name, row.GEOID20, block_prob.BLACK_share, block_prob.HISP_share, block_prob.ASIAN_share, block_prob.WHITE_share, block_prob.OTH_share] \n",
    "    assert math.isclose(sum(new_row[3:]), 1, abs_tol = .01)\n",
    "    voter_pred_df.loc[ix] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e9f30b0-3e67-4117-81c1-b34f1cc31f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_pred_df = pd.read_csv(\"Anderson-voter-predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64b25ded-f857-4b27-97b2-060baca10cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LALVOTERID</th>\n",
       "      <th>LASTNAME</th>\n",
       "      <th>GEOID20</th>\n",
       "      <th>BLACK_prob</th>\n",
       "      <th>HISP_prob</th>\n",
       "      <th>ASIAN_prob</th>\n",
       "      <th>WHITE_prob</th>\n",
       "      <th>OTH_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LALTX494641725</td>\n",
       "      <td>Hathcox</td>\n",
       "      <td>480039504001027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LALTX4462161</td>\n",
       "      <td>Trevino</td>\n",
       "      <td>480039504001049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LALTX1673620</td>\n",
       "      <td>Trevino</td>\n",
       "      <td>480039504001049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LALTX1053644</td>\n",
       "      <td>Trevino</td>\n",
       "      <td>480039504001049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LALTX1050354</td>\n",
       "      <td>Bartley</td>\n",
       "      <td>480039504001029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.915957</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>LALTX1032167</td>\n",
       "      <td>Ham</td>\n",
       "      <td>480039504001008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>LALTX1032133</td>\n",
       "      <td>Ham</td>\n",
       "      <td>480039504001008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>LALTX512731576</td>\n",
       "      <td>Perry</td>\n",
       "      <td>480039504001008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>LALTX513086922</td>\n",
       "      <td>Sutton</td>\n",
       "      <td>480039504001008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>LALTX494609591</td>\n",
       "      <td>Varela</td>\n",
       "      <td>480039504001002</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LALVOTERID LASTNAME          GEOID20  BLACK_prob  HISP_prob  \\\n",
       "0    LALTX494641725  Hathcox  480039504001027         0.0   1.000000   \n",
       "1      LALTX4462161  Trevino  480039504001049         0.0   0.978053   \n",
       "2      LALTX1673620  Trevino  480039504001049         0.0   0.978053   \n",
       "3      LALTX1053644  Trevino  480039504001049         0.0   0.978053   \n",
       "4      LALTX1050354  Bartley  480039504001029         0.0   0.084043   \n",
       "..              ...      ...              ...         ...        ...   \n",
       "135    LALTX1032167      Ham  480039504001008         0.2   0.200000   \n",
       "136    LALTX1032133      Ham  480039504001008         0.2   0.200000   \n",
       "137  LALTX512731576    Perry  480039504001008         0.2   0.200000   \n",
       "138  LALTX513086922   Sutton  480039504001008         0.2   0.200000   \n",
       "139  LALTX494609591   Varela  480039504001002         0.2   0.200000   \n",
       "\n",
       "     ASIAN_prob  WHITE_prob  OTH_prob  \n",
       "0           0.0    0.000000       0.0  \n",
       "1           0.0    0.021947       0.0  \n",
       "2           0.0    0.021947       0.0  \n",
       "3           0.0    0.021947       0.0  \n",
       "4           0.0    0.915957       0.0  \n",
       "..          ...         ...       ...  \n",
       "135         0.2    0.200000       0.2  \n",
       "136         0.2    0.200000       0.2  \n",
       "137         0.2    0.200000       0.2  \n",
       "138         0.2    0.200000       0.2  \n",
       "139         0.2    0.200000       0.2  \n",
       "\n",
       "[140 rows x 8 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd1b865c-b4a9-4aa8-9a5b-4428ac89c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21030\n"
     ]
    }
   ],
   "source": [
    "print(len(voter_pred_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4388fbdb-8618-4416-9b1c-c39d067bfe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15027\n",
      "10918\n",
      "18986\n",
      "18639\n"
     ]
    }
   ],
   "source": [
    "print(len(voter_pred_df[voter_pred_df.BLACK_prob > 0]))\n",
    "print(len(voter_pred_df[voter_pred_df.ASIAN_prob > 0]))\n",
    "print(len(voter_pred_df[voter_pred_df.OTH_prob > 0]))\n",
    "print(len(voter_pred_df[voter_pred_df.HISP_prob > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388daa46-4202-4327-b5c2-90b7a46e130c",
   "metadata": {},
   "source": [
    "Ok, this is both a test and the beginning of post-processing. \n",
    "\n",
    "With every voter now BISG-ed, we can re-merge to the anderson dataframe and aggregate by GEOID. \n",
    "\n",
    "I'm writing this after running the cells, and these numbers are somewhat believable, except the Asian probability predicts way more Asian voters than seem to be in Anderson county. \n",
    "\n",
    "Wikipedia supports that there's a small number of Asian voters in Anderson, so...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cae700e-d827-4caa-9c20-2cd1be972f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_pred_agg = anderson.merge(voter_pred_df, on = \"LALVOTERID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f818bd-b943-4cf9-810d-16dfe5de144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_pred_agg = voter_pred_agg[[\"GEOID20\", \"BLACK_prob\", \"HISP_prob\", \"ASIAN_prob\", \"OTH_prob\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a1bb9f2-f928-4c69-afe7-b3a126db465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_pred_agg = voter_pred_agg.groupby(\"GEOID20\").sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "028c2365-cb20-4bb2-b821-74d2b7f4ac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACK_prob: 3546.607179353943\n",
      "HISP_prob: 4545.687854497082\n",
      "ASIAN_prob: 2482.4867511668795\n",
      "OTH_prob: 10455.21821498209\n"
     ]
    }
   ],
   "source": [
    "for col in list(voter_pred_agg.columns[1:]):\n",
    "    print(f\"{col}: {sum(voter_pred_agg[col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdd4bdf4-ae15-4af3-ba07-361059b04858",
   "metadata": {},
   "outputs": [],
   "source": [
    "county = gpd.read_file(\"http://data.mggg.org.s3-website.us-east-2.amazonaws.com/census-2020/tx/tx_county.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f995e54c-8504-44d6-9ad6-fbecd1fc499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "county[\"EIOVAP20\"] = county[\"VAP20\"] - county[\"APBVAP20\"] - county[\"HVAP20\"] - county[\"ASIANVAP20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a291acc2-6f72-4eff-933a-b38e1bafa807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APBVAP20: 10146\n",
      "HVAP20: 8350\n",
      "ASIANVAP20: 317\n",
      "EIOVAP20: 28568\n"
     ]
    }
   ],
   "source": [
    "groups = [\"APBVAP20\", \"HVAP20\", \"ASIANVAP20\", \"EIOVAP20\"]\n",
    "for group in groups: \n",
    "    print(f\"{group}: {sum(county[group][county.NAME20 == 'Anderson'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92959e5c-5454-45a6-8399-76233807d5f3",
   "metadata": {},
   "source": [
    "# In theory the post processing is easy from here: \n",
    "    * We'll have BISG predictions on all counties for all election years. \n",
    "    * Concatenate each output to one large dataframe (only with the columns for GEOID20 and each probability)\n",
    "    * Separately I'll make a mapping from every election date they have to the elections in our data set. Then for each unique voter, we'll figure out which elections they participated in, and sum the total votes for each election. \n",
    "     *  For each election date, there will be new pop columns that map to a speciifc data, and the pop data to be used in EI will specifically be the population numbers for each group on that day. i.e. BVAP201103 (sum of Black Voters for the November 3, 2020 election)\n",
    "    * Aggregate from blocks to VTDs\n",
    "    * Merge with current Texas.csv being used for EI!\n",
    "    \n",
    "    * And then there will be some EI-side changes before this is run-able"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26271a86-8451-434d-bb52-f20fa693b191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
